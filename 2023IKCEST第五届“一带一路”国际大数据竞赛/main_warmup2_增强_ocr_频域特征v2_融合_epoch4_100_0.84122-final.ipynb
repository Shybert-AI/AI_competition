{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023IKCEST第五届“一带一路”国际大数据竞赛\n",
    "# 一、背景介绍\n",
    "\n",
    "本届大数据竞赛在中国工程院、教育部高等学校大学计算机课程教学指导委员会及丝绸之路大学联盟的指导下由联合国教科文组织国际工程科技知识中心（IKCEST）、中国工程科技知识中心（CKCEST）、百度公司及西安交通大学共同主办，旨在放眼“一带一路”倡议沿线国家，通过竞赛方式挖掘全球大数据人工智能尖端人才，实现政府—产业—高校合力推动大数据产业研究、应用、发展的目标，进一步夯实赛事的理论基础与实践基础，加快拔尖AI创新人才培养。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、赛题介绍\n",
    "随着新媒体时代信息媒介的多元化发展，各种内容大量活跃在媒体内中，与此同时各类虚假信息也充斥着社交媒体，影响着公众的判断和决策。如何在大量的文本、图像等多模态信息中，通过大数据与人工智能技术，纠正和消除虚假错误信息，对于网络舆情及社会治理有着重大意义。\n",
    "\n",
    "本次赛题要求选手基于官方指定数据集，通过建模同一事实跨模态数据之间的关系 （主要是文本和图像），实现对任一模态信息能够进行虚假和真实性的检测。鼓励参赛选手通过大模型解决问题，进行技术探索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:31.297345Z",
     "iopub.status.busy": "2023-10-20T15:55:31.296555Z",
     "iopub.status.idle": "2023-10-20T15:55:34.389669Z",
     "shell.execute_reply": "2023-10-20T15:55:34.388438Z",
     "shell.execute_reply.started": "2023-10-20T15:55:31.297296Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Requirement already satisfied: paddlenlp==2.4.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.4.2)\r\n",
      "Requirement already satisfied: multiprocess<=0.70.12.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.70.11.1)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.4.4)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.42.1)\r\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (1.1.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.1.96)\r\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (1.2.2)\r\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (1.0.0)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (4.1.0)\r\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (2.4.0)\r\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (0.3.3)\r\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (2.7.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.0,>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (3.20.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.4.2) (4.64.1)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (2022.11.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (5.1.2)\r\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (10.0.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (3.8.3)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (1.19.5)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (3.1.0)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (2.24.0)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (21.3)\r\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (0.18.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (4.2.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/aistudio/.data/webide/pip/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (0.16.4)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.4.2) (1.1.5)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.4.2) (0.24.2)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (1.16.0)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (0.8.53)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (2.2.3)\r\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (8.2.0)\r\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (1.1.1)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.4.2) (1.0.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (1.1.0)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (3.0.0)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (8.0.4)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.4.2) (0.16.0)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.4.2) (2019.3)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.4.2) (2.8.0)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (4.0.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (2.1.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (22.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (6.0.2)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (4.3.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (1.7.2)\r\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.4.2) (0.13.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets>=2.0.0->paddlenlp==2.4.2) (3.0.12)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp==2.4.2) (3.0.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (3.0.4)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (2.8)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (1.25.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.4.2) (2019.9.11)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (1.6.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.4.2) (0.14.1)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.4.2) (0.18.0)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.4.2) (3.9.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.4.2) (3.8.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.4.2) (2.8.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp==2.4.2) (2.0.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp==2.4.2) (56.2.0)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#环境安装\n",
    "!pip install paddlenlp==2.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.解压套件安装PaddleOCR套件\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/9e1e197487d1466fa662471a62f610b6357dad3b73ea4de698b59a020b4fed07)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:34.392547Z",
     "iopub.status.busy": "2023-10-20T15:55:34.391929Z",
     "iopub.status.idle": "2023-10-20T15:55:45.235038Z",
     "shell.execute_reply": "2023-10-20T15:55:45.233847Z",
     "shell.execute_reply.started": "2023-10-20T15:55:34.392511Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aistudio/data/data244320/PaddleOCR-2.6.0\r\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Requirement already satisfied: shapely in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.19.3)\r\n",
      "Requirement already satisfied: imgaug in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.4.0)\r\n",
      "Requirement already satisfied: pyclipper in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.3.0.post5)\r\n",
      "Requirement already satisfied: lmdb in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (1.4.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (4.64.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.19.5)\r\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (2.4.0)\r\n",
      "Requirement already satisfied: rapidfuzz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (3.4.0)\r\n",
      "Requirement already satisfied: opencv-contrib-python==4.4.0.46 in /home/aistudio/.data/webide/pip/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (4.4.0.46)\r\n",
      "Requirement already satisfied: cython in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 11)) (0.29)\r\n",
      "Requirement already satisfied: lxml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 12)) (4.9.1)\r\n",
      "Requirement already satisfied: premailer in /home/aistudio/.data/webide/pip/lib/python3.7/site-packages (from -r requirements.txt (line 13)) (3.10.0)\r\n",
      "Requirement already satisfied: openpyxl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from -r requirements.txt (line 14)) (3.0.5)\r\n",
      "Requirement already satisfied: attrdict in /home/aistudio/.data/webide/pip/lib/python3.7/site-packages (from -r requirements.txt (line 15)) (2.0.1)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.6.1)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (2.4)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.6.3)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (1.4.0)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (8.2.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (2021.11.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image->-r requirements.txt (line 2)) (21.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug->-r requirements.txt (line 3)) (1.16.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug->-r requirements.txt (line 3)) (2.2.3)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug->-r requirements.txt (line 3)) (4.6.0.66)\r\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->-r requirements.txt (line 8)) (2.24.0)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->-r requirements.txt (line 8)) (0.8.53)\r\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->-r requirements.txt (line 8)) (1.1.1)\r\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->-r requirements.txt (line 8)) (3.20.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->-r requirements.txt (line 8)) (1.1.5)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->-r requirements.txt (line 8)) (1.0.0)\r\n",
      "Requirement already satisfied: cssutils in /home/aistudio/.data/webide/pip/lib/python3.7/site-packages (from premailer->-r requirements.txt (line 13)) (2.7.1)\r\n",
      "Requirement already satisfied: cssselect in /home/aistudio/.data/webide/pip/lib/python3.7/site-packages (from premailer->-r requirements.txt (line 13)) (1.2.0)\r\n",
      "Requirement already satisfied: cachetools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from premailer->-r requirements.txt (line 13)) (4.0.0)\r\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from openpyxl->-r requirements.txt (line 14)) (1.0.1)\r\n",
      "Requirement already satisfied: jdcal in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from openpyxl->-r requirements.txt (line 14)) (1.4.1)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 8)) (8.0.4)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 8)) (1.1.0)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 8)) (0.16.0)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->-r requirements.txt (line 8)) (3.0.0)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->-r requirements.txt (line 8)) (2019.3)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->-r requirements.txt (line 8)) (2.8.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from networkx>=2.2->scikit-image->-r requirements.txt (line 2)) (4.4.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging>=20.0->scikit-image->-r requirements.txt (line 2)) (3.0.9)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->-r requirements.txt (line 8)) (0.18.0)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->-r requirements.txt (line 8)) (3.9.9)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from cssutils->premailer->-r requirements.txt (line 13)) (4.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug->-r requirements.txt (line 3)) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug->-r requirements.txt (line 3)) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug->-r requirements.txt (line 3)) (1.1.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->-r requirements.txt (line 8)) (2.8)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->-r requirements.txt (line 8)) (1.25.11)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->-r requirements.txt (line 8)) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->-r requirements.txt (line 8)) (2019.9.11)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->-r requirements.txt (line 8)) (2.0.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug->-r requirements.txt (line 3)) (56.2.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->cssutils->premailer->-r requirements.txt (line 13)) (4.3.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->cssutils->premailer->-r requirements.txt (line 13)) (3.8.1)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.19.3)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (2.4)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (1.4.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (2021.11.2)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (1.6.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (21.3)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (2.6.1)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (8.2.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image) (1.19.5)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from networkx>=2.2->scikit-image) (4.4.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging>=20.0->scikit-image) (3.0.9)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "Looking in indexes: https://mirror.baidu.com/pypi/simple, https://mirrors.aliyun.com/pypi/simple/, https://pypi.tuna.tsinghua.edu.cn/simple/\r\n",
      "Requirement already satisfied: imgaug in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (0.4.0)\r\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (1.19.5)\r\n",
      "Requirement already satisfied: imageio in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (2.6.1)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (8.2.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (2.2.3)\r\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (0.19.3)\r\n",
      "Requirement already satisfied: Shapely in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (2.0.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (1.6.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (1.16.0)\r\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from imgaug) (4.6.0.66)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (2021.11.2)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (1.4.0)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (2.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-image>=0.14.2->imgaug) (21.3)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug) (2019.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug) (2.8.2)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->imgaug) (3.0.9)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->imgaug) (56.2.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.14.2->imgaug) (4.4.2)\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n",
      "mkdir: 无法创建目录\"inference\": 文件已存在\r\n",
      "--2023-10-20 23:55:43--  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar\r\n",
      "正在解析主机 paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c04:1001:1002:0:ff:b001:368a\r\n",
      "正在连接 paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)|182.61.200.195|:443... 已连接。\r\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\r\n",
      "长度： 3829760 (3.7M) [application/x-tar]\r\n",
      "正在保存至: “ch_PP-OCRv3_det_infer.tar.11”\r\n",
      "\r\n",
      "ch_PP-OCRv3_det_inf 100%[===================>]   3.65M  5.08MB/s    in 0.7s    \r\n",
      "\r\n",
      "2023-10-20 23:55:44 (5.08 MB/s) - 已保存 “ch_PP-OCRv3_det_infer.tar.11” [3829760/3829760])\r\n",
      "\r\n",
      "--2023-10-20 23:55:44--  https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar\r\n",
      "正在解析主机 paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)... 182.61.200.229, 182.61.200.195, 2409:8c04:1001:1002:0:ff:b001:368a\r\n",
      "正在连接 paddleocr.bj.bcebos.com (paddleocr.bj.bcebos.com)|182.61.200.229|:443... 已连接。\r\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\r\n",
      "长度： 11909120 (11M) [application/x-tar]\r\n",
      "正在保存至: “ch_PP-OCRv3_rec_infer.tar.11”\r\n",
      "\r\n",
      "ch_PP-OCRv3_rec_inf 100%[===================>]  11.36M  22.0MB/s    in 0.5s    \r\n",
      "\r\n",
      "2023-10-20 23:55:45 (22.0 MB/s) - 已保存 “ch_PP-OCRv3_rec_infer.tar.11” [11909120/11909120])\r\n",
      "\r\n",
      "/home/aistudio/data/data244320\r\n"
     ]
    }
   ],
   "source": [
    "%cd PaddleOCR-2.6.0\n",
    "# 安装依赖库\n",
    "!pip install -r requirements.txt -i https://mirror.baidu.com/pypi/simple\n",
    "!pip install scikit-image -i https://mirror.baidu.com/pypi/simple\n",
    "!pip install imgaug -i https://mirror.baidu.com/pypi/simple\n",
    "!mkdir inference && cd PaddleOCR-2.6.0/inference\n",
    "# 下载8.6M中文模型的检测模型并解压\n",
    "! cd inference && wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_det_infer.tar && tar xf ch_PP-OCRv3_det_infer.tar\n",
    "# 下载8.6M中文模型的识别模型并解压\n",
    "! cd inference && wget https://paddleocr.bj.bcebos.com/PP-OCRv3/chinese/ch_PP-OCRv3_rec_infer.tar && tar xf ch_PP-OCRv3_rec_infer.tar\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、数据集介绍\n",
    "本次比赛提供从国内外主流社交媒体平台上爬取的含有不同领域声明的数据集。\n",
    "\n",
    "初赛：训练集与验证集： 提供中文训练集5694条以及英文数据4893条，同时公开英文验证集611条与中文验证集711条供选手优化模型。\n",
    "\n",
    "初赛评测数据： 提供文娱、经济、健康领域的测试数据，这些领域的数据较容易区分。英文与中文数据集的测试集各600条。参赛队伍上传的结果文本的每一行就是对应的分类结果，该数据不公布，用于评测。\n",
    "\n",
    "\n",
    "| 0 | 1 | 2 |\n",
    "| -------- | -------- | -------- |\n",
    "| non-rumor | rumor  | unverified |\n",
    "\n",
    "\n",
    "\n",
    "[复赛数据后续见官网通知](https://aistudio.baidu.com/aistudio/competition/detail/1030/0/task-definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、数据预处理\n",
    "**数据集过大，右键选择解压/home/aistudio/data/data229919/data.zip数据集，耐心等待30分钟，直到出现以下文件夹和文件,解压之后硬盘达到约80g（压缩包27g、解压文件之后50g，可以将项目挂载的数据集取消，空余出27g）**\n",
    "* test\n",
    "* train\n",
    "* val\n",
    "* dataset_items_test.json\n",
    "* dataset_items_train.json\n",
    "* dataset_items_val.json\n",
    "\n",
    "此处将数据集已经放置在queries_dataset_merge文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:45.237403Z",
     "iopub.status.busy": "2023-10-20T15:55:45.236678Z",
     "iopub.status.idle": "2023-10-20T15:55:49.056133Z",
     "shell.execute_reply": "2023-10-20T15:55:49.055025Z",
     "shell.execute_reply.started": "2023-10-20T15:55:45.237368Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import time\n",
    "import os \n",
    "import copy\n",
    "import json\n",
    "import gc\n",
    "from urllib.parse import urlparse\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm \n",
    "from PIL import Image\n",
    "import imghdr\n",
    "import logging\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "import paddle\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddle.vision import transforms as T\n",
    "from paddle.io import Dataset\n",
    "from paddle.io import DataLoader\n",
    "from paddle.vision import models\n",
    "from paddlenlp.transformers import ErnieMModel,ErnieMTokenizer\n",
    "from paddle.metric import Metric\n",
    "os.environ[\"FLAGS_eager_delete_tensor_gb\"] = \"0.0\"\n",
    "sys.path.insert(0, \"PaddleOCR-2.6.0\")\n",
    "import tools.infer.utility as utility\n",
    "import tools.infer.predict_rec as predict_rec\n",
    "import tools.infer.predict_det as predict_det\n",
    "import tools.infer.predict_cls as predict_cls\n",
    "from ppocr.utils.utility import get_image_file_list, check_and_read\n",
    "from ppocr.utils.logging import get_logger\n",
    "from tools.infer.utility import draw_ocr_box_txt, get_rotate_crop_image\n",
    "seed = 2021\n",
    "paddle.seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.059237Z",
     "iopub.status.busy": "2023-10-20T15:55:49.058334Z",
     "iopub.status.idle": "2023-10-20T15:55:49.302318Z",
     "shell.execute_reply": "2023-10-20T15:55:49.301188Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.059205Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#读取数据\n",
    "path = \"/home/aistudio/queries_dataset_merge\"\n",
    "data_items_train = json.load(open(os.path.join(path,\"dataset_items_train.json\")))\n",
    "data_items_val = json.load(open(os.path.join(path,\"dataset_items_val.json\")))\n",
    "data_items_test = json.load(open(os.path.join(path,\"dataset_items_val_60.json\")))\n",
    "train_list = glob.glob(os.path.join(path,\"train/img/*jpg\"))\n",
    "test_list = glob.glob(os.path.join(path,\"test/img/*jpg\"))\n",
    "val_list = glob.glob(os.path.join(path,\"val/img/*jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 ocr识别训练集、验证集、测试集中的img图片¶\n",
    "### 定义ocr生成图片文本的txt格式,代码修改与PaddleOCR-2.6.0/tools/infer/predict_system.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.304169Z",
     "iopub.status.busy": "2023-10-20T15:55:49.303872Z",
     "iopub.status.idle": "2023-10-20T15:55:49.645275Z",
     "shell.execute_reply": "2023-10-20T15:55:49.644019Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.304144Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #20小时\n",
    "# 定义函数，用于返回图片的文本信息\n",
    "!cp predict_system.py   PaddleOCR-2.6.0/tools/infer/\n",
    "def orc_image(image_pth):\n",
    "    cmd = f\"cd PaddleOCR-2.6.0 && python3 tools/infer/predict_system.py  --det_model_dir=inference/ch_PP-OCRv3_det_infer/ --rec_model_dir=inference/ch_PP-OCRv3_rec_infer/  --image_dir={image_pth} --draw_img_save_dir=ch_PP-OCRv2_results\"\n",
    "    os.system(cmd)\n",
    "    os.system(f\"mv PaddleOCR-2.6.0/ch_PP-OCRv2_results/system_results.txt {image_pth}.txt\")\n",
    "#ocr_image_text = orc_image(\"/home/aistudio/queries_dataset_merge/val/img/1044.jpg\")\n",
    "# print(ocr_image_text)\n",
    "for image_path in train_list:\n",
    "    if not os.path.isfile(image_path+\".txt\"):\n",
    "        orc_image(image_path)\n",
    "for image_path in test_list:\n",
    "    if not os.path.isfile(image_path+\".txt\"):\n",
    "        orc_image(image_path)\n",
    "for image_path in val_list:\n",
    "    if not os.path.isfile(image_path+\".txt\"):\n",
    "        print(image_path)\n",
    "        orc_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义频域特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.648050Z",
     "iopub.status.busy": "2023-10-20T15:55:49.647246Z",
     "iopub.status.idle": "2023-10-20T15:55:49.677340Z",
     "shell.execute_reply": "2023-10-20T15:55:49.676690Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.648014Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_dct = T.Compose([\n",
    "                    T.Resize((224,224)),\n",
    "                    T.ToTensor()\n",
    "            ])\n",
    "\n",
    "def process_dct_img(img):\n",
    "    img = img.numpy() #size = [1, 224, 224]\n",
    "    height = img.shape[1]\n",
    "    width = img.shape[2]\n",
    "    #print('height:{}'.format(height))\n",
    "    N = 8 \n",
    "    step = int(height/N) #28\n",
    "\n",
    "    dct_img = np.zeros((1, N*N, step*step, 1), dtype=np.float32) #[1,64,784,1]\n",
    "    fft_img = np.zeros((1, N*N, step*step, 1))\n",
    "    #print('dct_img:{}'.format(dct_img.shape))\n",
    "    \n",
    "    i = 0\n",
    "    for row in np.arange(0, height, step):\n",
    "        for col in np.arange(0, width, step):\n",
    "            block = np.array(img[:, row:(row+step), col:(col+step)], dtype=np.float32)\n",
    "            #print('block:{}'.format(block.shape))\n",
    "            block1 = block.reshape(-1, step*step, 1) #[batch_size,784,1]\n",
    "            dct_img[:, i,:,:] = dct(block1) #[batch_size, 64, 784, 1]\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    #for i in range(64):\n",
    "    fft_img[:,:,:,:] = fft(dct_img[:,:,:,:]).real #[batch_size,64, 784,1]\n",
    "    \n",
    "    fft_img = paddle.to_tensor(fft_img).astype('float32')#.float() #[batch_size, 64, 784, 1]\n",
    "    new_img = F.interpolate(fft_img, size=[250,1]) #[batch_size, 64, 250, 1]\n",
    "    new_img = new_img.squeeze(0).squeeze(-1) #torch.size = [64, 250]\n",
    "    \n",
    "    return new_img   \n",
    "\n",
    "class DctStem(nn.Layer):\n",
    "    def __init__(self, kernel_sizes, num_channels):\n",
    "        super(DctStem, self).__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "            ConvBNRelu2d(in_channels=1,\n",
    "                         out_channels=num_channels[0],\n",
    "                         kernel_size=kernel_sizes[0]),\n",
    "            ConvBNRelu2d(\n",
    "                in_channels=num_channels[0],\n",
    "                out_channels=num_channels[1],\n",
    "                kernel_size=kernel_sizes[1],\n",
    "            ),\n",
    "            ConvBNRelu2d(\n",
    "                in_channels=num_channels[1],\n",
    "                out_channels=num_channels[2],\n",
    "                kernel_size=kernel_sizes[2],\n",
    "            ),\n",
    "            nn.MaxPool2D((1, 2)),\n",
    "        )\n",
    "\n",
    "    def forward(self, dct_img):\n",
    "        x = dct_img.unsqueeze(1)\n",
    "        x = x.unsqueeze(1)\n",
    "        img = self.convs(x)\n",
    "        img = img.transpose([0, 2, 1, 3])\n",
    "\n",
    "        return img\n",
    "\n",
    "class DctInceptionBlock(nn.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channel=128,\n",
    "        branch1_channels=[64],\n",
    "        branch2_channels=[48, 64],\n",
    "        branch3_channels=[64, 96, 96],\n",
    "        branch4_channels=[32],\n",
    "    ):\n",
    "        super(DctInceptionBlock, self).__init__()\n",
    "\n",
    "        self.branch1 = ConvBNRelu2d(in_channels=in_channel,\n",
    "                                    out_channels=branch1_channels[0],\n",
    "                                    kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            ConvBNRelu2d(in_channels=in_channel,\n",
    "                         out_channels=branch2_channels[0],\n",
    "                         kernel_size=1),\n",
    "            ConvBNRelu2d(\n",
    "                in_channels=branch2_channels[0],\n",
    "                out_channels=branch2_channels[1],\n",
    "                kernel_size=3,\n",
    "                padding=(0, 1),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            ConvBNRelu2d(in_channels=in_channel,\n",
    "                         out_channels=branch3_channels[0],\n",
    "                         kernel_size=1),\n",
    "            ConvBNRelu2d(\n",
    "                in_channels=branch3_channels[0],\n",
    "                out_channels=branch3_channels[1],\n",
    "                kernel_size=3,\n",
    "                padding=(0, 1),\n",
    "            ),\n",
    "            ConvBNRelu2d(\n",
    "                in_channels=branch3_channels[1],\n",
    "                out_channels=branch3_channels[2],\n",
    "                kernel_size=3,\n",
    "                padding=(0, 1),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2D(kernel_size=(1, 3), stride=1, padding=(0, 1)),\n",
    "            ConvBNRelu2d(in_channels=in_channel,\n",
    "                         out_channels=branch4_channels[0],\n",
    "                         kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.transpose([0, 2, 1, 3])\n",
    "        # y = x\n",
    "        out1 = self.branch1(x)\n",
    "        out2 = self.branch2(x)\n",
    "        out3 = self.branch3(x)\n",
    "        out4 = self.branch4(x)\n",
    "        out = paddle.concat(x = [out1, out2, out3, out4], axis=1)\n",
    "        out = out.transpose([0, 2, 1, 3])\n",
    "\n",
    "        return out\n",
    "def ConvBNRelu2d(in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2D(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=(1, kernel_size),\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "        ),\n",
    "        nn.BatchNorm2D(out_channels),\n",
    "        nn.ReLU(name=True),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class DctCNN(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 model_dim,\n",
    "                 dropout,\n",
    "                 kernel_sizes,\n",
    "                 num_channels,\n",
    "                 in_channel=128,\n",
    "                 branch1_channels=[64],\n",
    "                 branch2_channels=[48, 64],\n",
    "                 branch3_channels=[64, 96, 96],\n",
    "                 branch4_channels=[32],\n",
    "                 out_channels=64):\n",
    "\n",
    "        super(DctCNN, self).__init__()\n",
    "\n",
    "        self.stem = DctStem(kernel_sizes, num_channels)\n",
    "\n",
    "        self.InceptionBlock = DctInceptionBlock(\n",
    "            in_channel,\n",
    "            branch1_channels,\n",
    "            branch2_channels,\n",
    "            branch3_channels,\n",
    "            branch4_channels,\n",
    "        )\n",
    "\n",
    "        self.maxPool = nn.MaxPool2D((1, 122))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv = ConvBNRelu2d(branch1_channels[-1] + branch2_channels[-1] +\n",
    "                               branch3_channels[-1] + branch4_channels[-1],\n",
    "                               out_channels,\n",
    "                               kernel_size=1)\n",
    "\n",
    "    def forward(self, dct_img):\n",
    "        dct_f = self.stem(dct_img)\n",
    "        x = self.InceptionBlock(dct_f)\n",
    "        x = self.maxPool(x)\n",
    "        x = x.transpose([0, 2, 1, 3])\n",
    "        x = self.conv(x)\n",
    "        x = x.transpose([0, 2, 1, 3])\n",
    "        x = x.squeeze(-1)\n",
    "        x = x.reshape([-1,4096])\n",
    "\n",
    "        return x\n",
    "    \n",
    "class NetShareFusion(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 kernel_sizes,\n",
    "                 num_channels,\n",
    "                 model_dim,\n",
    "                 drop_and_BN,\n",
    "                 num_labels=2,\n",
    "                 dropout=0.5):\n",
    "\n",
    "        super(NetShareFusion, self).__init__()\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.drop_and_BN = drop_and_BN\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #dct_image\n",
    "        self.dct_img = DctCNN(model_dim,\n",
    "                              dropout,\n",
    "                              kernel_sizes,\n",
    "                              num_channels,\n",
    "                              in_channel=128,\n",
    "                              branch1_channels=[64],\n",
    "                              branch2_channels=[48, 64],\n",
    "                              branch3_channels=[64, 96, 96],\n",
    "                              branch4_channels=[32],\n",
    "                              out_channels=64)\n",
    "        self.linear_dct = nn.Linear(4096, model_dim)\n",
    "        self.bn_dct = nn.BatchNorm1D(model_dim)\n",
    "\n",
    "       \n",
    "        #classifier\n",
    "        self.linear1 = nn.Linear(model_dim, 35)\n",
    "        self.bn_1 = nn.BatchNorm1D(35)\n",
    "        self.linear2 = nn.Linear(35, num_labels)\n",
    "        self.softmax = nn.Softmax(axis=1)\n",
    "    \n",
    "    def drop_BN_layer(self, x, part='dct'):\n",
    "        if part == 'dct':\n",
    "            bn = self.bn_dct\n",
    "\n",
    "        if self.drop_and_BN == 'drop-BN':\n",
    "            x = self.dropout(x)\n",
    "            x = bn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self,dct_img):\n",
    "\n",
    "        #dct_feature\n",
    "        dct_out = self.dct_img(dct_img)\n",
    "        dct_out = F.relu(self.linear_dct(dct_out))\n",
    "        dct_out = self.drop_BN_layer(dct_out, part='dct')\n",
    "\n",
    "        return dct_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据中的每一个样本：图像img、文本caption、对应的img_html_news、inverse_search为支持图像img和文本caption的证据材料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.679068Z",
     "iopub.status.busy": "2023-10-20T15:55:49.678824Z",
     "iopub.status.idle": "2023-10-20T15:55:49.703980Z",
     "shell.execute_reply": "2023-10-20T15:55:49.703340Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.679047Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_string(input_str):\n",
    "    input_str = input_str.replace('&#39;', ' ')\n",
    "    input_str = input_str.replace('<b>','')\n",
    "    input_str = input_str.replace('</b>','')\n",
    "    #input_str = unidecode(input_str)  \n",
    "    return input_str\n",
    "    \n",
    "class NewsContextDatasetEmbs(Dataset):\n",
    "    def __init__(self, context_data_items_dict, queries_root_dir, split):\n",
    "        self.context_data_items_dict = context_data_items_dict\n",
    "        self.queries_root_dir = queries_root_dir\n",
    "        self.idx_to_keys = list(context_data_items_dict.keys())\n",
    "        self.transform =T.Compose([\n",
    "                        T.Resize(256),\n",
    "                        T.CenterCrop(224),\n",
    "                        T.ToTensor(),\n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                    ])\n",
    "        self.split=split\n",
    "    def __len__(self):\n",
    "        return len(self.context_data_items_dict)   \n",
    "\n",
    "\n",
    "    def load_img_pil(self,image_path):\n",
    "        if imghdr.what(image_path) == 'gif': \n",
    "            try:\n",
    "                with open(image_path, 'rb') as f:\n",
    "                    img = Image.open(f)\n",
    "                    return img.convert('RGB')\n",
    "            except:\n",
    "                return None \n",
    "        with open(image_path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            return img.convert('RGB')\n",
    "    def load_imgs_direct_search(self,item_folder_path,direct_dict):   \n",
    "        list_imgs_tensors = []\n",
    "        count = 0   \n",
    "        keys_to_check = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys_to_check:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    image_path = os.path.join(item_folder_path,page['image_path'].split('/')[-1])\n",
    "                    try:\n",
    "                        pil_img = self.load_img_pil(image_path)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        print(image_path)\n",
    "                    if pil_img == None: continue\n",
    "                    transform_img = self.transform(pil_img)\n",
    "                    count = count + 1 \n",
    "                    list_imgs_tensors.append(transform_img)\n",
    "        stacked_tensors = paddle.stack(list_imgs_tensors, axis=0)\n",
    "        return stacked_tensors\n",
    "\n",
    "    def load_captions(self,inv_dict):\n",
    "        captions = ['']\n",
    "        pages_with_captions_keys = ['all_fully_matched_captions','all_partially_matched_captions']\n",
    "        for key1 in pages_with_captions_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        item = page['title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    \n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "                    \n",
    "        pages_with_title_only_keys = ['partially_matched_no_text','fully_matched_no_text']\n",
    "        for key1 in pages_with_title_only_keys:\n",
    "            if key1 in inv_dict.keys():\n",
    "                for page in inv_dict[key1]:\n",
    "                    if 'title' in page.keys():\n",
    "                        title = process_string(page['title'])\n",
    "                        captions.append(title)\n",
    "        return captions\n",
    "\n",
    "    def load_captions_weibo(self,direct_dict):\n",
    "        captions = ['']\n",
    "        keys = ['images_with_captions','images_with_no_captions','images_with_caption_matched_tags']\n",
    "        for key1 in keys:\n",
    "            if key1 in direct_dict.keys():\n",
    "                for page in direct_dict[key1]:\n",
    "                    if 'page_title' in page.keys():\n",
    "                        item = page['page_title']\n",
    "                        item = process_string(item)\n",
    "                        captions.append(item)\n",
    "                    if 'caption' in page.keys():\n",
    "                        sub_captions_list = []\n",
    "                        unfiltered_captions = []\n",
    "                        for key2 in page['caption']:\n",
    "                            sub_caption = page['caption'][key2]\n",
    "                            sub_caption_filter = process_string(sub_caption)\n",
    "                            if sub_caption in unfiltered_captions: continue \n",
    "                            sub_captions_list.append(sub_caption_filter) \n",
    "                            unfiltered_captions.append(sub_caption) \n",
    "                        captions = captions + sub_captions_list \n",
    "        #print(captions)\n",
    "        return captions\n",
    "        #加载img文件夹\n",
    "    def load_queries(self,key):\n",
    "        caption = self.context_data_items_dict[key]['caption']\n",
    "        image_path = os.path.join(self.queries_root_dir,self.context_data_items_dict[key]['image_path'])\n",
    "        pil_img = self.load_img_pil(image_path)\n",
    "        # ======================\n",
    "        with open(image_path+\".txt\") as f:\n",
    "            data =f.readlines()\n",
    "        text_all = []\n",
    "        if len(data) !=0:\n",
    "            for i in eval(data[0].split(\"\\t\")[1]):\n",
    "                text_all.append(i['transcription'])\n",
    "            ocr_image_text = [\",\".join(text_all)]\n",
    "        else:\n",
    "            ocr_image_text = [\"\"]\n",
    "        # ======================\n",
    "        dct_img = transform_dct(Image.open(image_path).convert('L'))\n",
    "        #ocr_image_text = [orc_image(image_path)]\n",
    "        transform_img = self.transform(pil_img)\n",
    "        return transform_img, caption,ocr_image_text,dct_img\n",
    "    def __getitem__(self, idx):\n",
    "        #print(idx)\n",
    "        #print(self.context_data_items_dict)      \n",
    "        #idx = idx.tolist()               \n",
    "        key = self.idx_to_keys[idx]\n",
    "        #print(key)\n",
    "        item=self.context_data_items_dict.get(str(key))\n",
    "        #print(item)\n",
    "        # 如果为test没有label属性\n",
    "        #print(self.split)\n",
    "        if self.split=='train' or self.split=='val':\n",
    "            label = paddle.to_tensor(int(item['label']))\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json')))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json')))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)  \n",
    "            #imgs_text = self.load_imgs_direct_search_text(direct_path_item,direct_dict)  \n",
    "            qImg,qCap,imgs_text,dct_img =  self.load_queries(key)\n",
    "            sample = {'label': label, 'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap,\"imgs_text\":imgs_text,\"dct_img\":dct_img}\n",
    "        else:\n",
    "            direct_path_item = os.path.join(self.queries_root_dir,item['direct_path'])\n",
    "            inverse_path_item = os.path.join(self.queries_root_dir,item['inv_path'])\n",
    "            inv_ann_dict = json.load(open(os.path.join(inverse_path_item, 'inverse_annotation.json')))\n",
    "            direct_dict = json.load(open(os.path.join(direct_path_item, 'direct_annotation.json')))\n",
    "            captions= self.load_captions(inv_ann_dict)\n",
    "            captions += self.load_captions_weibo(direct_dict)\n",
    "            imgs = self.load_imgs_direct_search(direct_path_item,direct_dict)    \n",
    "            #imgs_text = self.load_imgs_direct_search_text(direct_path_item,direct_dict) \n",
    "            qImg,qCap,imgs_text,dct_img =  self.load_queries(key)\n",
    "            sample = {'caption': captions,'imgs': imgs,  'qImg': qImg, 'qCap': qCap,\"imgs_text\":imgs_text,\"dct_img\":dct_img}\n",
    "        #print(sample)\n",
    "        #print(len(captions)) \n",
    "        #print(type(imgs))\n",
    "        #print(imgs.size)\n",
    "        #print(imgs.shape)  \n",
    "        return sample,  len(captions), imgs.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.705194Z",
     "iopub.status.busy": "2023-10-20T15:55:49.704844Z",
     "iopub.status.idle": "2023-10-20T15:55:49.711217Z",
     "shell.execute_reply": "2023-10-20T15:55:49.710589Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.705172Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### load Datasets ####\n",
    "train_dataset = NewsContextDatasetEmbs(data_items_train, path,'train')\n",
    "val_dataset = NewsContextDatasetEmbs(data_items_val,path,'val')\n",
    "test_dataset = NewsContextDatasetEmbs(data_items_test,path,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.712505Z",
     "iopub.status.busy": "2023-10-20T15:55:49.712107Z",
     "iopub.status.idle": "2023-10-20T15:55:49.727038Z",
     "shell.execute_reply": "2023-10-20T15:55:49.726398Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.712481Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_context_bert_train(batch):\n",
    "    #print(batch)\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    img_text_batch = []\n",
    "    dct_img_batch = []\n",
    "    cap_batch = []\n",
    "    labels = [] \n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        labels.append(sample['label'])\n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0], sample['imgs'].shape[1], sample['imgs'].shape[2], sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        #print(1)\n",
    "        img_batch.append(padded_mem_img)#pad证据图片\n",
    "        cap_batch.append(captions)\n",
    "        img_text_batch.append(sample['imgs_text'])\n",
    "        dct_img_batch.append(sample['dct_img'])\n",
    "        qImg_batch.append(sample['qImg'])#[3, 224, 224]\n",
    "        qCap_batch.append(sample['qCap'])     \n",
    "    #print(labels)   \n",
    "    #print(img_batch)\n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    labels = paddle.stack(labels, axis=0) \n",
    "    #print(3)  \n",
    "    return labels, cap_batch, img_batch, qCap_batch, qImg_batch,img_text_batch,dct_img_batch\n",
    "\n",
    "def collate_context_bert_test(batch):\n",
    "    samples = [item[0] for item in batch]\n",
    "    max_captions_len = max([item[1] for item in batch])\n",
    "    max_images_len = max([item[2] for item in batch])\n",
    "    qCap_batch = []\n",
    "    qImg_batch = []\n",
    "    img_batch = []\n",
    "    img_text_batch = []\n",
    "    dct_img_batch = []\n",
    "    cap_batch = []\n",
    "    for j in range(0,len(samples)):  \n",
    "        sample = samples[j]    \n",
    "        captions = sample['caption']\n",
    "        cap_len = len(captions)\n",
    "        for i in range(0,max_captions_len-cap_len):\n",
    "            captions.append(\"\")\n",
    "        if len(sample['imgs'].shape) > 2:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1],sample['imgs'].shape[2],sample['imgs'].shape[3])\n",
    "        else:\n",
    "            padding_size = (max_images_len-sample['imgs'].shape[0],sample['imgs'].shape[1])\n",
    "        padded_mem_img = paddle.concat((sample['imgs'], paddle.zeros(padding_size)),axis=0)\n",
    "        img_batch.append(padded_mem_img)\n",
    "        cap_batch.append(captions)\n",
    "        img_text_batch.append(sample['imgs_text'])\n",
    "        dct_img_batch.append(sample['dct_img'])\n",
    "        qImg_batch.append(sample['qImg'])\n",
    "        qCap_batch.append(sample['qCap'])        \n",
    "    img_batch = paddle.stack(img_batch, axis=0)\n",
    "    qImg_batch = paddle.stack(qImg_batch, axis=0)\n",
    "    return cap_batch, img_batch, qCap_batch, qImg_batch,img_text_batch,dct_img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.730109Z",
     "iopub.status.busy": "2023-10-20T15:55:49.729395Z",
     "iopub.status.idle": "2023-10-20T15:55:49.734301Z",
     "shell.execute_reply": "2023-10-20T15:55:49.733688Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.730085Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn = collate_context_bert_train, return_list=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn = collate_context_bert_train,  return_list=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn = collate_context_bert_test, return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型构建\n",
    "**本次赛题为一个NLP与多模态的分类赛题，整体建模采用特征提取、特征交互、预测分类三个阶段**\n",
    "\n",
    "**特征提取：** 对于图像数据，使用ResNet模型进行特征提取、对于文本数据，使用预训练模型Ernie-m多语言模型对中文和英文同时处理，qCap,qImg,（需要验证的标题或图像材料）、caps,imgs（支持验证的文本、图像证据材料）\n",
    "\n",
    "**特征交互**：使用多头自注意力机制，将标题与文本证据材料交互、图像与图像证据材料交互，输出与需要验证的标题和图像的相关证据特征caps_feature、imgs_features\n",
    "\n",
    "**预测分类：** 最后使用全连接层将标题特征、图像特征、相关的文本证据特征、相关的图像证据特征拼接输入到分类器得到最终结果\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/3f29e3f853b9445fbeb24189103cdbbcb8364498dc484593a891839994dadbd6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多语言预训练模型ERNIE-M\n",
    "2021年，百度发布多语言预训练模型ERNIE-M。ERNIE-M通过对96门语言的学习，使得一个模型能同时理解96种语言，该项技术在5类典型跨语言理解任务上刷新世界最好效果。\n",
    "\n",
    "## ERNIE-M原理\n",
    "ERNIE-M基于飞桨PaddlePaddle框架训练，该模型构建了大小为25万的多语言词表，涵盖了96种语言的大多数常见词汇，训练语料包含了汉语、英语、法语、南非语、阿尔巴尼亚语、阿姆哈拉语、梵语、阿拉伯语、亚美尼亚语、阿萨姆语、阿塞拜疆语等96种语言，约1.5万亿字符。\n",
    "\n",
    "ERNIE-M的学习过程由两阶段组成。第一阶段从少量的双语语料中学习跨语言理解能力，使模型学到初步的语言对齐关系；第二阶段使用回译的思想，通过大量的单语语料学习，增强模型的跨语言理解能力。\n",
    "\n",
    "[百度NLP知乎介绍](https://zhuanlan.zhihu.com/p/344810337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/5aad6df2b5b64d5bab0454299249c04e85cc227cf1be4352b09885670a2b7a07)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/baf24e95ce694cac8742e7075be7fcfd843609fc263f406cbece98149144619b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.735598Z",
     "iopub.status.busy": "2023-10-20T15:55:49.735371Z",
     "iopub.status.idle": "2023-10-20T15:55:49.771686Z",
     "shell.execute_reply": "2023-10-20T15:55:49.771075Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.735578Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Layer):\n",
    "    def __init__(self, resnet_arch = 'resnext101_64x4d'):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        if resnet_arch == 'resnet101':\n",
    "            resnet = models.resnet101(pretrained=True)\n",
    "        elif resnet_arch == 'resnext101_64x4d':\n",
    "            resnet = models.resnext101_64x4d(pretrained=True)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2D((1, 1))\n",
    "    def forward(self, images, features='pool'):\n",
    "        out = self.resnet(images)\n",
    "        if features == 'pool':\n",
    "            out = self.adaptive_pool(out)\n",
    "            out = paddle.reshape(out, (out.shape[0],out.shape[1]))\n",
    "        return out\n",
    "\n",
    "class NetWork(nn.Layer):\n",
    "    def __init__(self, mode):\n",
    "        super(NetWork, self).__init__()\n",
    "        self.mode = mode           \n",
    "        self.ernie = ErnieMModel.from_pretrained('ernie-m-base')\n",
    "        self.tokenizer = ErnieMTokenizer.from_pretrained('ernie-m-base')\n",
    "        self.resnet = EncoderCNN()\n",
    "        self.dct_feature_model = NetShareFusion(\n",
    "                kernel_sizes= [3, 3, 3],\n",
    "                num_channels=[32, 64, 128],\n",
    "                model_dim=256,\n",
    "                dropout=0.5,\n",
    "                drop_and_BN=['drop-BN'])\n",
    "        # 只能存在一个\n",
    "        self.ocr = False\n",
    "        self.dct = False\n",
    "        self.fusion_type = True\n",
    "        if self.dct:\n",
    "            self.classifier1 = nn.Linear(1*(768+2048)+768*3+256,1024) \n",
    "        if self.ocr:\n",
    "            self.classifier1 = nn.Linear(1*(768+2048)+768*4,1024) \n",
    "        self.classifier2 = nn.Linear(1024,3)\n",
    "        self.attention_text = nn.MultiHeadAttention(768,16)\n",
    "        self.attention_image = nn.MultiHeadAttention(2048,16)\n",
    "        self.attention_image_text = nn.MultiHeadAttention(768,4)\n",
    "        self.mid_attention_image_text = nn.Linear(2048,128*768) \n",
    "        self.mlp1 = paddle.nn.Sequential(\n",
    "                paddle.nn.Linear(2048*2+768*3, 1024),\n",
    "                paddle.nn.ReLU(),\n",
    "                paddle.nn.Dropout(0.2),\n",
    "                paddle.nn.Linear(1024, 3)\n",
    "            )\n",
    "\n",
    "        self.mlp2 = paddle.nn.Sequential(\n",
    "                paddle.nn.Linear(2048*1+768*4, 1024),\n",
    "                paddle.nn.ReLU(),\n",
    "                paddle.nn.Dropout(0.2),\n",
    "                paddle.nn.Linear(1024, 3)\n",
    "            )\n",
    "\n",
    "        self.mlp3 = paddle.nn.Sequential(\n",
    "                paddle.nn.Linear(768, 256),\n",
    "                paddle.nn.ReLU(),\n",
    "                paddle.nn.Dropout(0.2),\n",
    "                paddle.nn.Linear(256, 3)\n",
    "            )\n",
    "\n",
    "        self.mlp4 = paddle.nn.Sequential(\n",
    "                paddle.nn.Linear(2048, 1024),\n",
    "                paddle.nn.ReLU(),\n",
    "                paddle.nn.Dropout(0.2),\n",
    "                paddle.nn.Linear(1024, 3)\n",
    "            )\n",
    "\n",
    "        self.fusion_mlp = paddle.nn.Sequential(\n",
    "                        paddle.nn.Linear(2048, 768),\n",
    "                        paddle.nn.ReLU(),\n",
    "                        paddle.nn.Dropout(0.2),\n",
    "                    )\n",
    "        self.fusion_mlp2 = paddle.nn.Sequential(\n",
    "                paddle.nn.Linear(768, 256),\n",
    "                paddle.nn.ReLU(),\n",
    "                paddle.nn.Dropout(0.2),\n",
    "                paddle.nn.Linear(256, 3)\n",
    "            )\n",
    "        self.fusion_attention1 = nn.MultiHeadAttention(768,16)\n",
    "        self.fusion_attention2 = nn.MultiHeadAttention(768,16)\n",
    "        self.fusion_attention3 = nn.MultiHeadAttention(768,16)\n",
    "        self.fusion_attention4 = nn.MultiHeadAttention(768,16)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        if self.mode == 'text':\n",
    "            self.classifier = nn.Linear(768,3)\n",
    "        self.resnet.eval()\n",
    "\n",
    "    def forward(self,qCap,qImg,caps,imgs,img_text,dct_img):\n",
    "        self.resnet.eval()\n",
    "        encode_dict_qcap = self.tokenizer(text = qCap ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "        input_ids_qcap = encode_dict_qcap['input_ids']\n",
    "        input_ids_qcap = paddle.to_tensor(input_ids_qcap)\n",
    "        qcap_feature, pooled_output= self.ernie(input_ids_qcap) #(b,length,dim)\n",
    "        if self.mode == 'text':\n",
    "            logits = self.classifier(qcap_feature[:,0,:].squeeze(1))\n",
    "            return logits\n",
    "        caps_feature = []\n",
    "        for i,caption in enumerate (caps):\n",
    "            encode_dict_cap = self.tokenizer(text = caption ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "            input_ids_caps = encode_dict_cap['input_ids']\n",
    "            input_ids_caps = paddle.to_tensor(input_ids_caps)\n",
    "            cap_feature, pooled_output= self.ernie(input_ids_caps) #(b,length,dim)\n",
    "            caps_feature.append(cap_feature)\n",
    "        caps_feature = paddle.stack(caps_feature,axis=0) #(b,num,length,dim)\n",
    "        caps_feature = caps_feature.mean(axis=1)#(b,length,dim)\n",
    "        caps_feature_fusion = self.attention_text(qcap_feature,caps_feature,caps_feature) #(b,length,dim)\n",
    "        # =======================================================================\n",
    "        if True:#self.ocr:\n",
    "            img_text_feature = []\n",
    "            for i,img_caption in enumerate (img_text):\n",
    "                encode_dict_cap_img = self.tokenizer(text = img_caption ,max_length = 128 ,truncation=True, padding='max_length')\n",
    "                input_ids_caps_img = encode_dict_cap_img['input_ids']\n",
    "                input_ids_caps_img = paddle.to_tensor(input_ids_caps_img)\n",
    "                cap_img_feature, pooled_img_output= self.ernie(input_ids_caps_img) #(b,length,dim)\n",
    "                img_text_feature.append(cap_img_feature)\n",
    "            img_text_feature = paddle.stack(img_text_feature,axis=0) #(b,num,length,dim)\n",
    "            img_text_feature = img_text_feature.mean(axis=1)#(b,length,dim)\n",
    "            img_text_feature_qcap = self.attention_text(qcap_feature,img_text_feature,img_text_feature) #(b,length,dim)\n",
    "            img_text_feature_caps = self.attention_text(caps_feature,img_text_feature,img_text_feature) #(b,length,dim)\n",
    "        # =======================================================================\n",
    "        imgs_features = []\n",
    "        for img in imgs:\n",
    "            imgs_feature = self.resnet(img) #(length,dim)\n",
    "            imgs_features.append(imgs_feature)\n",
    "        imgs_features = paddle.stack(imgs_features,axis=0) #(b,length,dim)\n",
    "        qImg_features = []\n",
    "        for qImage in qImg:\n",
    "            qImg_feature = self.resnet(qImage.unsqueeze(axis=0)) #(1,dim)\n",
    "            qImg_features.append(qImg_feature)\n",
    "        qImg_feature = paddle.stack(qImg_features,axis=0) #(b,1,dim)\n",
    "        imgs_features = self.attention_image(qImg_feature,imgs_features,imgs_features) #(b,1,dim)\n",
    "        # 计算用的是融合后的图像信息，而不是imgs_feature,可能会有问题？\n",
    "        # 图像和文本证据融合\n",
    "        imgs_features_qcap = (self.mid_attention_image_text(imgs_features)).reshape([qcap_feature.shape[0],qcap_feature.shape[1],qcap_feature.shape[2]])\n",
    "        imgs_text_features_fusion_qcap = self.attention_image_text(imgs_features_qcap,imgs_features_qcap,qcap_feature)\n",
    "        # 图像和文本融合\n",
    "        imgs_features_caps = (self.mid_attention_image_text(imgs_features)).reshape([caps_feature.shape[0],caps_feature.shape[1],caps_feature.shape[2]])\n",
    "        imgs_text_features_fusion_caps = self.attention_image_text(imgs_features_caps,imgs_features_caps,caps_feature)\n",
    "        \n",
    "        if self.dct:\n",
    "            # ==================================\n",
    "            dct_feature_batch = []\n",
    "            for dct_img_ in dct_img:\n",
    "                dct_feature_ = process_dct_img(dct_img_)\n",
    "                dct_feature_ = self.dct_feature_model(dct_feature_)\n",
    "                dct_feature_batch.append(dct_feature_)\n",
    "\n",
    "            dct_feature = paddle.stack(dct_feature_batch,axis=0) #(b,num,length,dim)\n",
    "            dct_feature = dct_feature.mean(axis=1)#(b,length,dim)   \n",
    "            feature = paddle.concat(x=[caps_feature_fusion[:,0,:], imgs_text_features_fusion[:,0,:],imgs_features.squeeze(1),img_text_feature_qcap[:,0,:],img_text_feature_caps[:,0,:],dct_feature], axis=-1) \n",
    "        if self.ocr:\n",
    "            feature = paddle.concat(x=[caps_feature_fusion[:,0,:], imgs_text_features_fusion[:,0,:],imgs_features.squeeze(1),img_text_feature_qcap[:,0,:],img_text_feature_caps[:,0,:]], axis=-1) \n",
    "        #feature = paddle.concat(x=[qcap_feature[:,0,:], caps_feature[:,0,:], img_text_feature[:,0,:],qImg_feature.squeeze(1), imgs_features.squeeze(1),imgs_text_features[:,0,:]], axis=-1) \n",
    "        # logits = self.classifier1(feature)\n",
    "        # logits = self.classifier2(logits)\n",
    "        def fusion():\n",
    "            # 早期融合\n",
    "            feature1 = paddle.concat(x=[qcap_feature[:,0,:], caps_feature[:,0,:],imgs_features[:,0,:],qImg_feature[:,0,:],img_text_feature[:,0,:]], axis=-1) \n",
    "            logits1 = self.mlp1(feature1)\n",
    "            # 交叉融合\n",
    "            fusion1,fusion2,fusion3,fusion4,fusion5,fusion6 = caps_feature_fusion,imgs_text_features_fusion_qcap,imgs_features,imgs_text_features_fusion_caps,img_text_feature_qcap,img_text_feature_caps\n",
    "            if True:\n",
    "                fusion3 = self.fusion_mlp(fusion3)\n",
    "                fusion3 = paddle.tile(fusion3 , repeat_times=[128, 1])\n",
    "                \n",
    "                x1 = self.fusion_attention1(fusion1,fusion2,fusion2) + fusion1\n",
    "                x2 = self.fusion_attention1(fusion2,fusion3,fusion3) + fusion2\n",
    "                x3 = self.fusion_attention1(fusion3,fusion4,fusion4) + fusion3\n",
    "                x4 = self.fusion_attention1(fusion4,fusion5,fusion5) + fusion4\n",
    "                x5 = self.fusion_attention1(fusion5,fusion6,fusion6) + fusion6\n",
    "                x2_1 = self.fusion_attention2(x1,x2,x2) + x1\n",
    "                x2_2 = self.fusion_attention2(x2,x3,x3) + x2\n",
    "                x2_3 = self.fusion_attention2(x3,x4,x4) + x3\n",
    "                x2_4 = self.fusion_attention2(x4,x5,x5) + x4\n",
    "                x3_1 = self.fusion_attention3(x2_1,x2_2,x2_2) + x2_1\n",
    "                x3_2 = self.fusion_attention3(x2_2,x2_3,x2_3) + x2_2\n",
    "                x3_3 = self.fusion_attention3(x2_3,x2_4,x2_4) + x2_3\n",
    "                x4_1 = self.fusion_attention4(x3_1,x3_2,x3_2) + x3_1\n",
    "                x4_2 = self.fusion_attention4(x3_2,x3_3,x3_3) + x3_2\n",
    "                x5_1 = self.fusion_attention4(x4_1,x4_2,x4_2) + x4_1\n",
    "                logits2 = self.fusion_mlp2(x5_1[:,0,:])\n",
    "            else:\n",
    "                feature2 = paddle.concat(x=[fusion1,fusion2,fusion3.squeeze(1),fusion4,fusion5,fusion6], axis=-1) \n",
    "                logits2 = self.mlp2(feature2)\n",
    "            # 5个模态的特征，经过全连接层，进行晚期融合\n",
    "            muti1 = self.mlp3(qcap_feature[:,0,:])\n",
    "            muti2 = self.mlp3(caps_feature[:,0,:])\n",
    "            muti3 = self.mlp4(imgs_features[:,0,:])\n",
    "            muti4 = self.mlp4(qImg_feature[:,0,:])\n",
    "            muti5 = self.mlp3(img_text_feature[:,0,:])\n",
    "            logits3 =  muti1 + muti2 + muti3 + muti4 + muti5\n",
    "            # 三种方式进行融合，还不知系数，因此先都为1\n",
    "\n",
    "            logits = logits2+logits1 + logits3\n",
    "            return logits\n",
    "\n",
    "        \n",
    "        if self.fusion_type:\n",
    "            logits = fusion()\n",
    "        else:\n",
    "            # logits = self.classifier1(feature)\n",
    "            # logits = self.classifier2(logits)\n",
    "            logits = F.relu(self.classifier1(feature))\n",
    "            logits = self.dropout(logits)\n",
    "            #logits = self.classifier1(feature)\n",
    "            #output = self.bn_1(output)\n",
    "            logits = self.classifier2(logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:49.772941Z",
     "iopub.status.busy": "2023-10-20T15:55:49.772494Z",
     "iopub.status.idle": "2023-10-20T15:55:58.036438Z",
     "shell.execute_reply": "2023-10-20T15:55:58.034902Z",
     "shell.execute_reply.started": "2023-10-20T15:55:49.772919Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-20 23:55:49,776] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-m-base/ernie_m_base.pdparams\r\n",
      "W1020 23:55:49.781824 17873 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2\r\n",
      "W1020 23:55:49.786636 17873 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.\r\n",
      "[2023-10-20 23:55:55,484] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-m-base/ernie_m.vocab.txt\r\n",
      "[2023-10-20 23:55:55,487] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-m-base/ernie_m.sentencepiece.bpe.model\r\n",
      "[2023-10-20 23:55:56,377] [    INFO] - tokenizer config file saved in /home/aistudio/.paddlenlp/models/ernie-m-base/tokenizer_config.json\r\n",
      "[2023-10-20 23:55:56,380] [    INFO] - Special tokens file saved in /home/aistudio/.paddlenlp/models/ernie-m-base/special_tokens_map.json\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetWork(\r\n",
      "  (ernie): ErnieMModel(\r\n",
      "    (embeddings): ErnieMEmbeddings(\r\n",
      "      (word_embeddings): Embedding(250002, 768, sparse=False)\r\n",
      "      (position_embeddings): Embedding(514, 768, sparse=False)\r\n",
      "      (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "      (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "    )\r\n",
      "    (encoder): TransformerEncoder(\r\n",
      "      (layers): LayerList(\r\n",
      "        (0): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (1): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (2): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (3): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (4): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (5): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (6): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (7): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (8): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (9): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (10): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "        (11): TransformerEncoderLayer(\r\n",
      "          (self_attn): MultiHeadAttention(\r\n",
      "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "          )\r\n",
      "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\r\n",
      "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\r\n",
      "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\r\n",
      "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-05)\r\n",
      "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\r\n",
      "        )\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (pooler): ErnieMPooler(\r\n",
      "      (dense): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "      (activation): Tanh()\r\n",
      "    )\r\n",
      "  )\r\n",
      "  (resnet): EncoderCNN(\r\n",
      "    (resnet): Sequential(\r\n",
      "      (0): Conv2D(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=3, data_format=NCHW)\r\n",
      "      (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "      (2): ReLU()\r\n",
      "      (3): MaxPool2D(kernel_size=3, stride=2, padding=1)\r\n",
      "      (4): Sequential(\r\n",
      "        (0): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "          (downsample): Sequential(\r\n",
      "            (0): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (1): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (2): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(256, 256, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(256, 256, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (5): Sequential(\r\n",
      "        (0): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(256, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(512, 512, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "          (downsample): Sequential(\r\n",
      "            (0): Conv2D(256, 512, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (1): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (2): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (3): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(512, 512, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(512, 512, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (6): Sequential(\r\n",
      "        (0): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(512, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "          (downsample): Sequential(\r\n",
      "            (0): Conv2D(512, 1024, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (1): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (2): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (3): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (4): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (5): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (6): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (7): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (8): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (9): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (10): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (11): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (12): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (13): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (14): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (15): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (16): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (17): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (18): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (19): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (20): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (21): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (22): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(1024, 1024, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(1024, 1024, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (7): Sequential(\r\n",
      "        (0): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(1024, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(2048, 2048, kernel_size=[3, 3], stride=[2, 2], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(2048, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "          (downsample): Sequential(\r\n",
      "            (0): Conv2D(1024, 2048, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (1): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(2048, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(2048, 2048, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(2048, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "        (2): BottleneckBlock(\r\n",
      "          (conv1): Conv2D(2048, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn1): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv2): Conv2D(2048, 2048, kernel_size=[3, 3], padding=1, groups=64, data_format=NCHW)\r\n",
      "          (bn2): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (conv3): Conv2D(2048, 2048, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (bn3): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\r\n",
      "          (relu): ReLU()\r\n",
      "        )\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (adaptive_pool): AdaptiveAvgPool2D(output_size=(1, 1))\r\n",
      "  )\r\n",
      "  (dct_feature_model): NetShareFusion(\r\n",
      "    (dropout): Dropout(p=0.5, axis=None, mode=upscale_in_train)\r\n",
      "    (dct_img): DctCNN(\r\n",
      "      (stem): DctStem(\r\n",
      "        (convs): Sequential(\r\n",
      "          (0): Sequential(\r\n",
      "            (0): Conv2D(1, 32, kernel_size=[1, 3], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=32, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "          (1): Sequential(\r\n",
      "            (0): Conv2D(32, 64, kernel_size=[1, 3], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "          (2): Sequential(\r\n",
      "            (0): Conv2D(64, 128, kernel_size=[1, 3], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "          (3): MaxPool2D(kernel_size=(1, 2), stride=None, padding=0)\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (InceptionBlock): DctInceptionBlock(\r\n",
      "        (branch1): Sequential(\r\n",
      "          (0): Conv2D(128, 64, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "          (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "          (2): ReLU(name=True)\r\n",
      "        )\r\n",
      "        (branch2): Sequential(\r\n",
      "          (0): Sequential(\r\n",
      "            (0): Conv2D(128, 48, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=48, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "          (1): Sequential(\r\n",
      "            (0): Conv2D(48, 64, kernel_size=[1, 3], padding=(0, 1), data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (branch3): Sequential(\r\n",
      "          (0): Sequential(\r\n",
      "            (0): Conv2D(128, 64, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "          (1): Sequential(\r\n",
      "            (0): Conv2D(64, 96, kernel_size=[1, 3], padding=(0, 1), data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=96, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "          (2): Sequential(\r\n",
      "            (0): Conv2D(96, 96, kernel_size=[1, 3], padding=(0, 1), data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=96, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "        )\r\n",
      "        (branch4): Sequential(\r\n",
      "          (0): MaxPool2D(kernel_size=(1, 3), stride=1, padding=(0, 1))\r\n",
      "          (1): Sequential(\r\n",
      "            (0): Conv2D(128, 32, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "            (1): BatchNorm2D(num_features=32, momentum=0.9, epsilon=1e-05)\r\n",
      "            (2): ReLU(name=True)\r\n",
      "          )\r\n",
      "        )\r\n",
      "      )\r\n",
      "      (maxPool): MaxPool2D(kernel_size=(1, 122), stride=None, padding=0)\r\n",
      "      (dropout): Dropout(p=0.5, axis=None, mode=upscale_in_train)\r\n",
      "      (conv): Sequential(\r\n",
      "        (0): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)\r\n",
      "        (1): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\r\n",
      "        (2): ReLU(name=True)\r\n",
      "      )\r\n",
      "    )\r\n",
      "    (linear_dct): Linear(in_features=4096, out_features=256, dtype=float32)\r\n",
      "    (bn_dct): BatchNorm1D(num_features=256, momentum=0.9, epsilon=1e-05, data_format=NCL)\r\n",
      "    (linear1): Linear(in_features=256, out_features=35, dtype=float32)\r\n",
      "    (bn_1): BatchNorm1D(num_features=35, momentum=0.9, epsilon=1e-05, data_format=NCL)\r\n",
      "    (linear2): Linear(in_features=35, out_features=2, dtype=float32)\r\n",
      "    (softmax): Softmax(axis=1)\r\n",
      "  )\r\n",
      "  (classifier2): Linear(in_features=1024, out_features=3, dtype=float32)\r\n",
      "  (attention_text): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "  )\r\n",
      "  (attention_image): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=2048, out_features=2048, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=2048, out_features=2048, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=2048, out_features=2048, dtype=float32)\r\n",
      "  )\r\n",
      "  (attention_image_text): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "  )\r\n",
      "  (mid_attention_image_text): Linear(in_features=2048, out_features=98304, dtype=float32)\r\n",
      "  (mlp1): Sequential(\r\n",
      "    (0): Linear(in_features=6400, out_features=1024, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\r\n",
      "    (3): Linear(in_features=1024, out_features=3, dtype=float32)\r\n",
      "  )\r\n",
      "  (mlp2): Sequential(\r\n",
      "    (0): Linear(in_features=5120, out_features=1024, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\r\n",
      "    (3): Linear(in_features=1024, out_features=3, dtype=float32)\r\n",
      "  )\r\n",
      "  (mlp3): Sequential(\r\n",
      "    (0): Linear(in_features=768, out_features=256, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\r\n",
      "    (3): Linear(in_features=256, out_features=3, dtype=float32)\r\n",
      "  )\r\n",
      "  (mlp4): Sequential(\r\n",
      "    (0): Linear(in_features=2048, out_features=1024, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\r\n",
      "    (3): Linear(in_features=1024, out_features=3, dtype=float32)\r\n",
      "  )\r\n",
      "  (fusion_mlp): Sequential(\r\n",
      "    (0): Linear(in_features=2048, out_features=768, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\r\n",
      "  )\r\n",
      "  (fusion_mlp2): Sequential(\r\n",
      "    (0): Linear(in_features=768, out_features=256, dtype=float32)\r\n",
      "    (1): ReLU()\r\n",
      "    (2): Dropout(p=0.2, axis=None, mode=upscale_in_train)\r\n",
      "    (3): Linear(in_features=256, out_features=3, dtype=float32)\r\n",
      "  )\r\n",
      "  (fusion_attention1): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "  )\r\n",
      "  (fusion_attention2): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "  )\r\n",
      "  (fusion_attention3): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "  )\r\n",
      "  (fusion_attention4): MultiHeadAttention(\r\n",
      "    (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "    (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\r\n",
      "  )\r\n",
      "  (dropout): Dropout(p=0.5, axis=None, mode=upscale_in_train)\r\n",
      ")\r\n"
     ]
    }
   ],
   "source": [
    "# 声明模型\n",
    "model = NetWork(\"image\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:58.039674Z",
     "iopub.status.busy": "2023-10-20T15:55:58.038583Z",
     "iopub.status.idle": "2023-10-20T15:55:58.060710Z",
     "shell.execute_reply": "2023-10-20T15:55:58.059644Z",
     "shell.execute_reply.started": "2023-10-20T15:55:58.039618Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22368 1118\r\n"
     ]
    }
   ],
   "source": [
    "epochs = 4  \n",
    "num_training_steps = len(train_dataloader) * epochs\n",
    "warmup_steps = int(num_training_steps*0.1)\n",
    "warmup_steps = 1118# 1118  # len(train_dataloader) * 2 * 0.1\n",
    "print(num_training_steps,warmup_steps)\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(1e-5, num_training_steps, warmup_steps)\n",
    "# 训练结束后，存储模型参数\n",
    "save_dir =\"checkpoint/\"\n",
    "best_dir = \"best_model\"\n",
    "# 创建保存的文件夹\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "os.makedirs(best_dir,exist_ok=True)\n",
    "\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=1.2e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:58.062890Z",
     "iopub.status.busy": "2023-10-20T15:55:58.062244Z",
     "iopub.status.idle": "2023-10-20T15:55:58.092532Z",
     "shell.execute_reply": "2023-10-20T15:55:58.091715Z",
     "shell.execute_reply.started": "2023-10-20T15:55:58.062844Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "# 自定义MultiLabelReport评价指标\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "    def __init__(self, name='MultiLabelReport', average='micro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        metrics = {}\n",
    "        metrics[\"accuracy\"] = accuracy_score(y_true=self.y_true, y_pred=self.y_prob)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true=self.y_true, y_pred=self.y_prob,average=\"macro\")\n",
    "        average=\"macro\"\n",
    "        metrics[f\"{average}_precision\"] = precision\n",
    "        metrics[f\"{average}_recall\"] = recall\n",
    "        metrics[f\"{average}_f1\"] = f1\n",
    "        return metrics\n",
    "\n",
    "\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name\n",
    "metric2 = MultiLabelReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:58.094463Z",
     "iopub.status.busy": "2023-10-20T15:55:58.093958Z",
     "iopub.status.idle": "2023-10-20T15:55:58.103568Z",
     "shell.execute_reply": "2023-10-20T15:55:58.102485Z",
     "shell.execute_reply.started": "2023-10-20T15:55:58.094422Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义线下评估 评价指标为acc 线上评估是f1score\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader,epoch):\n",
    "    gc.collect()\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    metric2.reset()\n",
    "    losses = []\n",
    "    kkk = 1 \n",
    "    for batch in data_loader:    \n",
    "        print(kkk)  \n",
    "        kkk += 1\n",
    "        labels, cap_batch, img_batch, qCap_batch, qImg_batch,img_text_batch,dct_img_batch = batch\n",
    "        logits = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch,img_text=img_text_batch,dct_img = dct_img_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "        probs = F.softmax(logits)\n",
    "        metric2.update(labels,paddle.argmax(probs, axis=-1).reshape([-1,1]))\n",
    "    eval_f1_score = metric2.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    metric2.reset()\n",
    "    return np.mean(losses), accu,eval_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据增强  \n",
    " https://zhuanlan.zhihu.com/p/546801916  \n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/96824b62c24a4fa6b60ed70658059a172d5c23b2c6d942f0b3b2e03b5e4a193d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:58.105844Z",
     "iopub.status.busy": "2023-10-20T15:55:58.105393Z",
     "iopub.status.idle": "2023-10-20T15:55:58.111858Z",
     "shell.execute_reply": "2023-10-20T15:55:58.111068Z",
     "shell.execute_reply.started": "2023-10-20T15:55:58.105805Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mixgen(image,text,M,gama):\n",
    "    for i in range(M):\n",
    "        # image mixup\n",
    "        image[i:] = (gama*image[i:]+(1-gama)*image[i+M])\n",
    "        # text concatention\n",
    "        text[i] = text[i] + text[i+M]\n",
    "        return image,text\n",
    "# print(cap_batch)\n",
    "# img_batch,cap_batch = mixgen(img_batch,cap_batch,int(img_batch.shape[0]/2),0.5)\n",
    "# print(cap_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:55:58.113457Z",
     "iopub.status.busy": "2023-10-20T15:55:58.113064Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train run start\r\n"
     ]
    }
   ],
   "source": [
    "# 定义训练\n",
    "# param_dict = paddle.load('checkpoint/v3_epoch4_0.84115/model_state.pdparams')\n",
    "# model.load_dict(param_dict)\n",
    "# del param_dict\n",
    "def do_train(model, criterion, metric, val_dataloader,train_dataloader):\n",
    "    print(\"train run start\")\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    best_accuracy=0.0\n",
    "    # 梯度累计步\n",
    "    gradient_accumulation_step = 2\n",
    "    macro_f1 = []\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for step, batch in enumerate(train_dataloader, start=1): \n",
    "            #paddle.device.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            labels, cap_batch, img_batch, qCap_batch, qImg_batch,img_text_batch,dct_img_batch = batch\n",
    "            if False:#random.random() <0.5: # 用于控制增强\n",
    "                img_batch,cap_batch = mixgen(img_batch,cap_batch,int(img_batch.shape[0]/2),0.5)  \n",
    "                # 为了应对batch中末尾的text没有拼接，给没有增强的加上空值，使其每个batch里的text维度一样\n",
    "                num_max = max([len(i) for i in cap_batch]) \n",
    "                num_max = 50 if num_max>50 else num_max\n",
    "                cap_batch = [j+[\"\"]*(num_max-len(j)) if len(j) < num_max else j[:num_max] for j in cap_batch]\n",
    "            probs = model(qCap=qCap_batch,qImg=qImg_batch,caps=cap_batch,imgs=img_batch,img_text=img_text_batch,dct_img = dct_img_batch)\n",
    "            loss = criterion(probs, labels)/gradient_accumulation_step\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "            # 输出类别的概率\n",
    "            probs = F.softmax(probs)\n",
    "            metric2.update(labels,paddle.argmax(probs, axis=-1).reshape([-1,1]))\n",
    "            f1_score = metric2.accumulate()\n",
    "            macro_f1.append(round(f1_score[\"macro_f1\"],4))\n",
    "            train_loss.append(round(loss.item(),4))\n",
    "            train_acc.append(round(acc,4))\n",
    "            # 画指标\n",
    "            if global_step % 10 == 0:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(3, 1, 1)\n",
    "                plt.plot(macro_f1,color='r')\n",
    "                plt.ylabel('val/ macro_f1')\n",
    "                plt.title('macro_f1')\n",
    "                plt.subplot(3, 1, 2)\n",
    "                plt.plot(train_loss,color='b')\n",
    "                plt.ylabel('train_loss')\n",
    "                plt.title('train_loss')\n",
    "                plt.subplot(3, 1, 3)\n",
    "                plt.plot(train_acc,color='g')\n",
    "                plt.ylabel('train_acc')\n",
    "                plt.xlabel('step')\n",
    "                plt.title('train_acc')\n",
    "                plt.savefig(f'train_acc.png')\n",
    "            global_step += 1 \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, f1_score: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,f1_score['macro_f1'],\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "           \n",
    "            if (step+1) % gradient_accumulation_step == 0:  \n",
    "                optimizer.step()   # 更新参数                                       \n",
    "                lr_scheduler.step()\n",
    "                optimizer.clear_grad() #梯度清零\n",
    "                paddle.device.cuda.empty_cache()\n",
    "            # 保存最优模型\n",
    "            if global_step  == 16876:\n",
    "                save_param_path = os.path.join(\"model\", 'model_state.pdparams')\n",
    "                print(save_param_path)\n",
    "                paddle.save(model.state_dict(), save_param_path)\n",
    "            # 每间隔一个epoch 在验证集进行评估\n",
    "            if global_step % len(train_dataloader) == 0:\n",
    "                eval_loss,eval_accu,eval_f1_score=evaluate(model, criterion, metric, val_dataloader,epoch)\n",
    "                # save_param_path = os.path.join(save_dir+str(epoch), 'model_state.pdparams')\n",
    "                # paddle.save(model.state_dict(), save_param_path)\n",
    "                if(best_accuracy<eval_accu):\n",
    "                    best_accuracy=eval_accu\n",
    "                    # 保存模型\n",
    "                    #save_param_path = os.path.join(best_dir, 'model_best.pdparams')\n",
    "                    paddle.save(model.state_dict(), save_param_path)\n",
    "do_train(model, criterion, metric, val_dataloader,train_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 压缩\r\n",
    "#zip -r submission.zip  ernie-m-base  model  PaddleOCR-2.6.0  paddlenlp paddlenlp-2.4.2.dist-info predict.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T15:58:02.876446Z",
     "iopub.status.busy": "2023-10-20T15:58:02.875659Z",
     "iopub.status.idle": "2023-10-20T15:58:03.110670Z",
     "shell.execute_reply": "2023-10-20T15:58:03.109358Z",
     "shell.execute_reply.started": "2023-10-20T15:58:02.876396Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总用量 3375848\r\n",
      "drwxr-xr-x  3 aistudio aistudio       4096 10月 20 21:41 ernie-m-base\r\n",
      "-rw-r--r--  1 aistudio aistudio     148346 10月 20 23:58 main_warmup2_增强_ocr_频域特征v2_融合_epoch4_100_0.84122-final.ipynb\r\n",
      "drwxr-xr-x  3 aistudio aistudio       4096 10月 20 21:41 model\r\n",
      "drwxr-xr-x 18 aistudio aistudio       4096 10月 20 21:41 paddlenlp\r\n",
      "drwxr-xr-x  2 aistudio aistudio       4096 10月 20 21:41 paddlenlp-2.4.2.dist-info\r\n",
      "drwxr-xr-x 19 aistudio aistudio       4096 10月 20 23:01 PaddleOCR-2.6.0\r\n",
      "-rw-r--r--  1 aistudio aistudio      33819 10月 20 22:12 predict.py\r\n",
      "-rw-r--r--  1 aistudio aistudio       8393 10月 20 23:08 predict_system.py\r\n",
      "-rw-r--r--  1 aistudio aistudio       3015 10月 20 23:35 README.md\r\n",
      "-rw-r--r--  1 aistudio aistudio        282 10月 20 23:50 requirements.txt\r\n",
      "-rw-r--r--  1 aistudio aistudio         98 10月 20 22:04 result.csv\r\n",
      "-rw-r--r--  1 aistudio aistudio 3456548582 10月 20 22:16 submission.zip\r\n",
      "-rw-r--r--  1 aistudio aistudio      74681 10月 20 23:57 train_acc.png\r\n",
      "-rw-r--r--  1 aistudio aistudio        248 10月 20 22:07 团队信息.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-20T16:03:39.228881Z",
     "iopub.status.busy": "2023-10-20T16:03:39.228136Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: ernie-m-base/ (stored 0%)\r\n",
      "  adding: ernie-m-base/model_config.json (deflated 43%)\r\n",
      "  adding: ernie-m-base/tokenizer_config.json (deflated 34%)\r\n",
      "  adding: ernie-m-base/special_tokens_map.json (deflated 40%)\r\n",
      "  adding: ernie-m-base/vocab.txt (deflated 47%)\r\n",
      "  adding: ernie-m-base/ernie_m.sentencepiece.bpe.model (deflated 49%)\r\n",
      "  adding: ernie-m-base/.ipynb_checkpoints/ (stored 0%)\r\n",
      "  adding: ernie-m-base/.ipynb_checkpoints/model_config-checkpoint.json (deflated 43%)\r\n",
      "  adding: ernie-m-base/.ipynb_checkpoints/vocab-checkpoint.txt (deflated 47%)\r\n",
      "  adding: ernie-m-base/.ipynb_checkpoints/ernie_m.vocab-checkpoint.txt (deflated 47%)\r\n",
      "  adding: ernie-m-base/ernie_m.vocab.txt (deflated 47%)\r\n",
      "  adding: ernie-m-base/model_state.pdparams"
     ]
    }
   ],
   "source": [
    "!zip -r 傻瓜oop815的团队.zip  ernie-m-base main_warmup2_增强_ocr_频域特征v2_融合_epoch4_100_0.84122-final.ipynb  model    paddlenlp paddlenlp-2.4.2.dist-info predict.py PaddleOCR-2.6.0  predict.py   predict_system.py  README.md   requirements.txt result.csv submission.zip  团队信息.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
